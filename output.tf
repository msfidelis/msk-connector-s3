output "connect_configuration" {
  value = <<LOGS
connector.class=io.io.confluent.connect.s3.S3SinkConnector
flush.size=100
format.class=io.confluent.connect.s3.format.bytearray.ByteArrayFormat
path.format=YYYY/MM/dd
retry.backoff.ms=1000
s3.bucket.name=${aws_s3_bucket.topic_sync.id}
s3.compression.level=9
s3.region=us-east-1
topics.dir=topics
topics.regex=(.*)-logs$
tasks.max=2
key.converter.schemas.enable=false
errors.log.enable=true
store.kafka.keys=false
timezone=America/Sao_Paulo
locale=Pt-BR
directory.delim=/
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
storage.class=io.confluent.s3.storage.S3Storage
timestamp.extractor=Record
behavior.on.null.values=ignore
schema.ignore=true


connector.class=io.io.confluent.connect.s3.S3SinkConnector
behavior.on.null.values=ignore
filename.offset.zero.pad.width=10
s3.region=sa-east-1
topics.dir=topics
flush.size=1000
s3.compression.level=9
tasks.max=4
timezone=America/Sao_Paulo
locale=pt-BR
retry.backoff.ms=5000
format.class=io.confluent.connect.s3.format.bytearray.ByteArrayFormat
s3.part.retries=3
format.bytearray.extension=.json
s3.bucket.name=${aws_s3_bucket.topic_sync.id}
s3.retry.backoff.ms=200
store.kafka.keys=false
partition.duration.ms=600000
s3.ssea.name=aws:kms
file.delim=+
schema.compatibility=NONE
directory.delim=/
connect.meta.data=false
topics.regex=(.*)-logs$
store.kafka.headers=false
s3.sse.kms.key.id=${aws_kms_key.kms.id}
s3.compression.type=gzip
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
storage.class=io.confluent.connect.s3.storage.S3Storage
path.format='year'=YYYY/'month'=MM/'day'=dd
timestamp.extractor=Record
LOGS 
}
